{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "df_stamp:          date  month  day  weekday  hour\n",
      "0  2018-01-02      1    2        1     0\n",
      "1  2018-01-03      1    3        2     0\n",
      "2  2018-01-04      1    4        3     0\n",
      "3  2018-01-05      1    5        4     0\n",
      "4  2018-01-08      1    8        0     0\n",
      "5  2018-01-09      1    9        1     0\n",
      "6  2018-01-10      1   10        2     0\n",
      "7  2018-01-11      1   11        3     0\n",
      "8  2018-01-12      1   12        4     0\n",
      "9  2018-01-15      1   15        0     0\n",
      "10 2018-01-16      1   16        1     0\n",
      "11 2018-01-17      1   17        2     0\n",
      "12 2018-01-18      1   18        3     0\n",
      "13 2018-01-19      1   19        4     0\n",
      "14 2018-01-22      1   22        0     0\n",
      "15 2018-01-23      1   23        1     0\n",
      "16 2018-01-24      1   24        2     0\n",
      "17 2018-01-25      1   25        3     0\n",
      "18 2018-01-26      1   26        4     0\n",
      "19 2018-01-29      1   29        0     0\n",
      "Data shape:  (320, 1)\n",
      "Stamp shape:  (320, 4)\n",
      "Sample: [0.6170156] [0.6170156] [1 2 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\何锦洋\\AppData\\Local\\Temp\\ipykernel_46408\\1751489724.py:59: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
      "C:\\Users\\何锦洋\\AppData\\Local\\Temp\\ipykernel_46408\\1751489724.py:60: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
      "C:\\Users\\何锦洋\\AppData\\Local\\Temp\\ipykernel_46408\\1751489724.py:61: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
      "C:\\Users\\何锦洋\\AppData\\Local\\Temp\\ipykernel_46408\\1751489724.py:62: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.6170156 ],\n",
       "        [0.78608729],\n",
       "        [0.83219776],\n",
       "        [0.83219776]]),\n",
       " array([[0.83219776],\n",
       "        [1.61607559]]),\n",
       " array([[1, 2, 1, 0],\n",
       "        [1, 3, 2, 0],\n",
       "        [1, 4, 3, 0],\n",
       "        [1, 5, 4, 0]], dtype=int64),\n",
       " array([[1, 5, 4, 0],\n",
       "        [1, 8, 0, 0]], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dataset_Stock_hour(Dataset):\n",
    "    def __init__(self, args, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='../stock_data/train_set/ST世茂.csv',\n",
    "                 target='open', scale=True, timeenc=0, freq='d', seasonal_patterns=None):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        self.args = args\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 4\n",
    "            self.label_len = 1\n",
    "            self.pred_len = 1\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        print(\"Loading data ...\")\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        border1s = [0, 16 * 20 - self.seq_len, 16 * 20 + 6 * 20 - self.seq_len]\n",
    "        border2s = [16 * 20, 16 * 20 + 6 * 20, 16 * 20 + 8 * 20]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        # ... 处理时间标签\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            print(\"df_stamp:\", df_stamp.head(20))\n",
    "            data_stamp = df_stamp.drop(['date'], axis=1).values\n",
    "        elif self.timeenc == 1:\n",
    "            # data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0) \n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "\n",
    "        # if self.set_type == 0 and self.args.augmentation_ratio > 0:\n",
    "            # self.data_x, self.data_y, augmentation_tags = run_augmentation_single(self.data_x, self.data_y, self.args)\n",
    "\n",
    "        self.data_stamp = data_stamp\n",
    "        print(\"Data shape: \", self.data_x.shape)\n",
    "        print(\"Stamp shape: \", self.data_stamp.shape)\n",
    "        print(\"Sample:\", self.data_x[0], self.data_y[0], self.data_stamp[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "    \n",
    "\n",
    "# timeenc选择0\n",
    "test = Dataset_Stock_hour(None, './', flag='train')\n",
    "\n",
    "test.__getitem__(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
